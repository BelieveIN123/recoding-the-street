# recoding-the-street
В этом проекте были такие этапы: 
1) Найти где хранятся данные. 
2) Разобраться какое качество у этих данных. (По началу я пропустил этот этап и обучал, и проверял на данных, где в "у" были изначально не верные результаты. После, использовал для обучения только само название, т.е. для каждого класса был 1 обучающий пример равный названию класса.)
3) Этап предобработки. Нахождение наиболее встречаемых ошибок. Убираю не важные слова. 
Представление в каком формате. Я буду классифицировать наименования. Для большего сходства я решил, отсортировать названия по внутри. Так фразы по типу «улица» оказались бы с одной стороны, как в обучающем примере, так и в тестовом. 
4) Попробовал разные варианты векторизации и классификации. Заметил, что у меня недостаточно ресурсов на более сложные модели. Поэтому был применён Байесовский классификатор. Векторизация была сделана по 2-3 буквы от слова. Это хорошо сочеталась с отсортированными данными по длине слов. 
5) Проверка качества моделей. Происходила от руки. Простое сравнения то что получилось, с реальным названием.
6) Сделал срез, по отсортированным по predict_probe данным, предполагая на глаз, что «99.99%» улиц, будут перекодированы правильно. 

# Улучшения. 
Если бы я сейчас решал эту задачу я бы сделала 
1)	BPE – dropout токенизацию (возможно не хватило бы ресурсов) 
Что это: 
https://www.youtube.com/watch?v=lPhiSRz-2N0
2)	Создал бы 200-300 примеров в которых я уверен. И проверял бы качество моделей на них. 
